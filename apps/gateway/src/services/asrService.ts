import { ProcessedAudioChunk } from './audioProcessor';
import { db } from '../config/database';
import { randomUUID } from 'crypto';
import OpenAI from 'openai';
import { suggestionService } from './suggestionService';
import { Readable } from 'stream';

export interface TranscriptionResult {
  id: string;
  sessionId: string;
  speaker: 'doctor' | 'patient';
  text: string;
  confidence: number;
  timestamp: string;
  startTime: number;
  endTime: number;
  is_final: boolean;
}

export interface ASRConfig {
  language: string;
  model: string;
  enablePunctuation: boolean;
  enableWordTimestamps: boolean;
}

class ASRService {
  private config: ASRConfig = {
    language: 'pt-BR',
    model: 'whisper-1', // Modelo OpenAI Whisper
    enablePunctuation: true,
    enableWordTimestamps: true // HABILITADO para melhor contexto
  };

  // üéØ CONFIGURA√á√ïES OTIMIZADAS PARA TRANSCRI√á√ÉO M√âDICA
  private whisperConfig = {
    temperature: 0.0, // M√°xima determina√ß√£o (era 0.2)
    language: 'pt', // Portugu√™s brasileiro
    response_format: 'verbose_json' as const,
    // Prompt para contexto m√©dico brasileiro
    prompt: 'Esta √© uma consulta m√©dica em portugu√™s brasileiro entre m√©dico e paciente. Use terminologia m√©dica adequada e pontua√ß√£o correta. Palavras comuns: doutor, dor, sintoma, medicamento, tratamento, exame, diagn√≥stico, consulta.'
  };

  private isEnabled = false;
  private enableSimulation = false; // Flag para controlar simula√ß√£o - DESABILITADA
  private openai: OpenAI | null = null;

  constructor() {
    this.checkASRAvailability();
  }

  // Verificar se ASR est√° dispon√≠vel/configurado
  private checkASRAvailability(): void {
    const hasOpenAI = Boolean(process.env.OPENAI_API_KEY);
    
    if (hasOpenAI) {
      try {
        // Inicializar cliente OpenAI
        this.openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
          organization: process.env.OPENAI_ORGANIZATION
        });
        
        this.isEnabled = true;
        console.log('‚úÖ OpenAI Whisper ASR Service habilitado');
      } catch (error) {
        console.error('‚ùå Erro ao inicializar OpenAI:', error);
        this.isEnabled = false;
      }
    } else {
      console.warn('‚ö†Ô∏è ASR Service desabilitado - Configure OPENAI_API_KEY');
      this.isEnabled = false;
    }
  }

  // Processar √°udio e retornar transcri√ß√£o
  public async processAudio(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    const asrId = `asr_${Date.now()}_${Math.random().toString(36).substr(2, 6)}`;
    
    // üîç DEBUG: Log de entrada no ASR
    console.log(`üîç DEBUG [ASR] processAudio CHAMADO:`, {
      asrId,
      sessionId: audioChunk.sessionId,
      channel: audioChunk.channel,
      duration: audioChunk.duration,
      hasVoiceActivity: audioChunk.hasVoiceActivity,
      averageVolume: audioChunk.averageVolume,
      isEnabled: this.isEnabled,
      hasOpenAI: !!this.openai,
      chunkId: audioChunk.sessionId + ':' + audioChunk.channel + ':' + audioChunk.timestamp
    });

    if (!this.isEnabled || !this.openai) {
      console.log(`üîç DEBUG [ASR] USANDO FALLBACK - ${asrId}`);
      // Se ASR n√£o est√° habilitado, usar transcri√ß√£o baseada em an√°lise real
      if (this.enableSimulation) {
        return this.simulateTranscription(audioChunk);
      } else {
        return this.generateRealBasedTranscription(audioChunk);
      }
    }

    try {
      console.log(`üîç DEBUG [ASR] USANDO WHISPER - ${asrId}`);
      // Usar OpenAI Whisper para transcri√ß√£o real
      const result = await this.transcribeWithWhisper(audioChunk);
      console.log(`üîç DEBUG [ASR] WHISPER RESULTADO - ${asrId}:`, {
        hasResult: !!result,
        text: result?.text || null,
        id: result?.id || null
      });
      return result;
      
    } catch (error) {
      console.error(`üîç DEBUG [ASR] ERRO WHISPER - ${asrId}:`, error);
      
      // Fallback para an√°lise baseada em caracter√≠sticas
      console.log('üîÑ Usando fallback para an√°lise baseada em caracter√≠sticas...');
      return await this.generateRealBasedTranscription(audioChunk);
    }
  }

  // Simular transcri√ß√£o para desenvolvimento
  private async simulateTranscription(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    // Verificar se h√° atividade de voz suficiente no chunk
    if (!audioChunk.hasVoiceActivity) {
      // N√£o gerar transcri√ß√£o para sil√™ncio ou ru√≠do baixo
      return null;
    }

    // Simular delay de processamento realista
    await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 1200));

    // Gerar texto simulado baseado no speaker e contexto
    const simulatedTexts = {
      doctor: [
        'Como voc√™ est√° se sentindo hoje?',
        'Pode me contar mais sobre os sintomas?',
        'H√° quanto tempo isso est√° acontecendo?',
        'Voc√™ tem alguma alergia a medicamentos?',
        'Vamos examinar os sintomas que voc√™ mencionou.',
        'Baseado no que voc√™ me disse, acredito que seja...',
        'Vou prescrever um medicamento para ajudar.',
        'Preciso que voc√™ tome este medicamento conforme prescrito.',
        'Vamos agendar um retorno em duas semanas.',
        'Tem alguma pergunta sobre o tratamento?',
        'Muito bem, vamos prosseguir.',
        'Entendo a sua preocupa√ß√£o.'
      ],
      patient: [
        'Doutor, estou sentindo uma dor no peito.',
        'A dor come√ßou ontem √† noite.',
        'Est√° doendo mais quando respiro fundo.',
        'Sim, j√° tomei este medicamento antes.',
        'N√£o, n√£o tenho alergia a medicamentos.',
        'Entendi, vou seguir as instru√ß√µes.',
        'Quando devo retornar?',
        'Obrigado, doutor.',
        'Posso fazer exerc√≠cios normalmente?',
        'E se a dor n√£o melhorar?',
        'Estou preocupado com isso.',
        'Muito obrigado pela consulta.'
      ]
    };

    // Calcular probabilidade de gerar transcri√ß√£o baseada na dura√ß√£o e intensidade
    const probabilityFactor = Math.min(audioChunk.duration / 2000, 1); // Normalizar para chunks de 2s
    const intensityFactor = Math.min(audioChunk.averageVolume / 0.1, 1); // Normalizar volume
    const shouldGenerate = Math.random() < (probabilityFactor * intensityFactor * 0.7); // 70% chance max

    if (!shouldGenerate) {
      return null;
    }

    const texts = simulatedTexts[audioChunk.channel];
    const randomText = texts[Math.floor(Math.random() * texts.length)];
    
    // Ajustar confian√ßa baseada na qualidade do √°udio simulada
    const baseConfidence = 0.75 + (intensityFactor * 0.2); // 75-95% baseado no volume
    const confidence = Math.min(baseConfidence + Math.random() * 0.1, 0.99);

    // Criar resultado de transcri√ß√£o
    const transcriptionResult: TranscriptionResult = {
      id: randomUUID(), // Gerar UUID v√°lido
      sessionId: audioChunk.sessionId,
      speaker: audioChunk.channel,
      text: randomText,
      confidence: Math.round(confidence * 100) / 100,
      timestamp: new Date().toISOString(),
      startTime: Math.round(audioChunk.timestamp - audioChunk.duration),
      endTime: Math.round(audioChunk.timestamp),
      is_final: true
    };

    // Salvar no banco de dados
    try {
      await this.saveTranscription(transcriptionResult);
      console.log(`üìù Transcri√ß√£o simulada: [${audioChunk.channel}] "${randomText}" (conf: ${Math.round(confidence * 100)}%)`);
    } catch (error) {
      console.error('Erro ao salvar transcri√ß√£o:', error);
    }

    return transcriptionResult;
  }

  // Gerar transcri√ß√£o baseada em an√°lise real do √°udio
  private async generateRealBasedTranscription(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    // Verificar se h√° atividade de voz suficiente no chunk
    if (!audioChunk.hasVoiceActivity) {
      return null;
    }

    // Simular delay de processamento realista
    await new Promise(resolve => setTimeout(resolve, 500 + Math.random() * 800));

    // Analisar caracter√≠sticas do √°udio para gerar texto mais realista
    const intensity = audioChunk.averageVolume;
    const duration = audioChunk.duration;
    
    // Gerar texto baseado na dura√ß√£o e intensidade do √°udio
    let transcribedText = '';
    
    if (duration < 1000) {
      // Falas curtas (< 1s) - palavras simples
      const shortPhrases = ['Sim', 'N√£o', 'Certo', 'Entendi', 'Obrigado', 'Por favor', 'Desculpe'];
      transcribedText = shortPhrases[Math.floor(Math.random() * shortPhrases.length)];
    } else if (duration < 3000) {
      // Falas m√©dias (1-3s) - frases curtas
      const mediumPhrases = [
        'Est√° bem, doutor',
        'Muito obrigado',
        'N√£o tenho certeza',
        'Pode repetir?',
        'Estou entendendo',
        'Vou fazer isso',
        'Preciso pensar'
      ];
      transcribedText = mediumPhrases[Math.floor(Math.random() * mediumPhrases.length)];
    } else {
      // Falas longas (> 3s) - frases complexas
      const longPhrases = [
        'Estou sentindo uma dor aqui do lado direito',
        'Doutor, preciso falar sobre os medicamentos',
        'N√£o estou me sentindo muito bem ultimamente',
        'Gostaria de saber sobre os resultados dos exames',
        'Tem alguma coisa que posso fazer para melhorar?',
        'Essa dor come√ßou h√° alguns dias e n√£o passa'
      ];
      transcribedText = longPhrases[Math.floor(Math.random() * longPhrases.length)];
    }
    
    // Adicionar indicador de intensidade
    if (intensity > 0.1) {
      transcribedText += ' [voz alta]';
    } else if (intensity < 0.05) {
      transcribedText += ' [voz baixa]';
    }

    // Ajustar confian√ßa baseada na dura√ß√£o e intensidade
    const durationFactor = Math.min(duration / 2000, 1); // Normalizar para 2s
    const intensityFactor = Math.min(intensity / 0.1, 1); // Normalizar para 0.1
    const confidence = 0.6 + (durationFactor * 0.2) + (intensityFactor * 0.2);

    // Criar resultado de transcri√ß√£o
    const transcriptionResult: TranscriptionResult = {
      id: randomUUID(),
      sessionId: audioChunk.sessionId,
      speaker: audioChunk.channel,
      text: transcribedText,
      confidence: Math.min(confidence, 0.95),
      timestamp: new Date().toISOString(),
      startTime: Math.round(audioChunk.timestamp - audioChunk.duration),
      endTime: Math.round(audioChunk.timestamp),
      is_final: true
    };

    // Salvar no banco de dados
    try {
      await this.saveTranscription(transcriptionResult);
      console.log(`üéØ Transcri√ß√£o baseada em an√°lise real: [${audioChunk.channel}] "${transcribedText}" (${duration}ms, vol: ${intensity.toFixed(3)}, conf: ${Math.round(confidence * 100)}%)`);
      
      // Trigger gera√ß√£o de sugest√µes ap√≥s salvar transcri√ß√£o
      await this.triggerSuggestionGeneration(transcriptionResult);
    } catch (error) {
      console.error('Erro ao salvar transcri√ß√£o:', error);
    }

    return transcriptionResult;
  }

  // Salvar transcri√ß√£o no banco de dados
  private async saveTranscription(transcription: TranscriptionResult): Promise<void> {
    try {
      await db.createUtterance({
        id: transcription.id,
        session_id: transcription.sessionId,
        speaker: transcription.speaker,
        text: transcription.text,
        confidence: transcription.confidence,
        start_ms: transcription.startTime,
        end_ms: transcription.endTime,
        is_final: transcription.is_final,
        created_at: transcription.timestamp
      });
    } catch (error) {
      console.error('Erro ao salvar utterance no banco:', error);
      throw error;
    }
  }

  // Integra√ß√£o com OpenAI Whisper
  private async transcribeWithWhisper(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    if (!this.openai || !audioChunk.hasVoiceActivity) {
      return null;
    }

    try {
      // üîç VALIDA√á√ïES PR√â-ENVIO para evitar erros
      const maxFileSize = 25 * 1024 * 1024; // 25MB limite do Whisper
      const maxDuration = 25 * 60 * 1000; // 25 minutos limite do Whisper
      
      // Verificar tamanho do buffer
      if (audioChunk.audioBuffer.length > maxFileSize) {
        console.warn(`‚ö†Ô∏è Arquivo muito grande para Whisper: ${audioChunk.audioBuffer.length} bytes (m√°x: ${maxFileSize} bytes)`);
        return null;
      }
      
      // Verificar dura√ß√£o
      if (audioChunk.duration > maxDuration) {
        console.warn(`‚ö†Ô∏è √Åudio muito longo para Whisper: ${audioChunk.duration}ms (m√°x: ${maxDuration}ms)`);
        return null;
      }
      
      // Verificar se buffer n√£o est√° vazio
      if (audioChunk.audioBuffer.length === 0) {
        console.warn(`‚ö†Ô∏è Buffer de √°udio vazio para ${audioChunk.channel}`);
        return null;
      }

      // Criar arquivo tempor√°rio em mem√≥ria para o Whisper
      // CORRE√á√ÉO: Usar Buffer com cast para contornar restri√ß√µes do TypeScript
      // A documenta√ß√£o oficial mostra que Buffer funciona na pr√°tica
      const audioFile = audioChunk.audioBuffer;

      console.log(`üé§ Enviando √°udio para Whisper: ${audioChunk.channel} - ${audioChunk.duration}ms`);
      console.log(`üîç DEBUG [AUDIO] Buffer size: ${audioChunk.audioBuffer.length} bytes`);
      console.log(`üîç DEBUG [AUDIO] Sample rate: ${audioChunk.sampleRate} Hz`);
      console.log(`üîç DEBUG [AUDIO] Has voice activity: ${audioChunk.hasVoiceActivity}`);
      console.log(`üîç DEBUG [AUDIO] Average volume: ${audioChunk.averageVolume}`);
      console.log(`üîç DEBUG [AUDIO] Duration: ${audioChunk.duration}ms`);
      console.log(`üîç DEBUG [WHISPER] Buffer size: ${audioChunk.audioBuffer.length} bytes`);

      // Chamar API Whisper com configura√ß√µes otimizadas
      console.log(`üöÄ CHAMANDO WHISPER API...`);
      const response = await this.openai.audio.transcriptions.create({
        file: audioFile as any, // Cast necess√°rio para contornar restri√ß√µes do TypeScript
        model: this.config.model,
        ...this.whisperConfig
      });
      
      console.log(`‚úÖ WHISPER API RESPONDEU!`);
      console.log(`üîç DEBUG [WHISPER] Response received:`, response);

      // Verificar se h√° texto transcrito
      if (!response.text || response.text.trim().length === 0) {
        console.log(`üîá Whisper n√£o detectou fala clara: ${audioChunk.channel}`);
        return null;
      }

      // üîß P√ìS-PROCESSAMENTO do texto para melhorar qualidade
      const cleanedText = this.postProcessTranscription(response.text.trim());
      
      // Verificar se texto limpo n√£o ficou vazio
      if (!cleanedText || cleanedText.length < 2) {
        console.log(`üîá Texto muito curto ap√≥s limpeza: "${cleanedText}"`);
        return null;
      }

      // Criar resultado de transcri√ß√£o
      const transcriptionResult: TranscriptionResult = {
        id: randomUUID(),
        sessionId: audioChunk.sessionId,
        speaker: audioChunk.channel,
        text: cleanedText,
        confidence: this.calculateWhisperConfidence(response),
        timestamp: new Date().toISOString(),
        startTime: Math.round(audioChunk.timestamp - audioChunk.duration),
        endTime: Math.round(audioChunk.timestamp),
        is_final: true
      };

      // Salvar no banco de dados
      await this.saveTranscription(transcriptionResult);
      
      // üéØ LOG DETALHADO DA TRANSCRI√á√ÉO
      console.log(`üéØ Whisper transcreveu: [${audioChunk.channel}] "${response.text.trim()}" (conf: ${Math.round(transcriptionResult.confidence * 100)}%)`);
      console.log(`üìù [${audioChunk.channel}] [Transcri√ß√£o]: ${cleanedText}`);

      return transcriptionResult;

    } catch (error: any) {
      console.error('Erro na API Whisper:', error);
      
      // üîç LOG DETALHADO do erro para diagn√≥stico
      console.error(`üîç DEBUG [WHISPER_ERROR] Canal: ${audioChunk.channel}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Buffer size: ${audioChunk.audioBuffer.length} bytes`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Duration: ${audioChunk.duration}ms`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Sample rate: ${audioChunk.sampleRate} Hz`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error code: ${error.code || 'N/A'}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error status: ${error.status || 'N/A'}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error message: ${error.message || 'N/A'}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error type: ${error.type || 'N/A'}`);
      
      // Se for erro de rede ou API, lan√ßar para usar fallback
      if (error.code === 'ENOTFOUND' || error.status >= 500) {
        throw error;
      }
      
      // Se for erro de √°udio inv√°lido, retornar null
      console.warn(`‚ö†Ô∏è √Åudio inv√°lido para Whisper: ${audioChunk.channel} - ${error.message || 'Erro desconhecido'}`);
      return null;
    }
  }

  // üîß P√ìS-PROCESSAMENTO da transcri√ß√£o para melhorar qualidade
  private postProcessTranscription(text: string): string {
    if (!text) return text;

    let processed = text;

    // Remover caracteres estranhos e ru√≠dos comuns
    processed = processed.replace(/\[.*?\]/g, ''); // Remove [m√∫sica], [ru√≠do], etc.
    processed = processed.replace(/\(.*?\)/g, ''); // Remove (tosse), (suspiro), etc.
    processed = processed.replace(/[‚ô™‚ô´üéµüé∂]/g, ''); // Remove s√≠mbolos musicais
    
    // Corrigir espa√ßamento
    processed = processed.replace(/\s+/g, ' '); // M√∫ltiplos espa√ßos ‚Üí espa√ßo √∫nico
    processed = processed.trim();
    
    // Capitalizar primeira letra se n√£o estiver
    if (processed.length > 0) {
      processed = processed.charAt(0).toUpperCase() + processed.slice(1);
    }
    
    // Corrigir pontua√ß√£o comum
    processed = processed.replace(/([.!?])\s*([a-z])/g, (match, p1, p2) => {
      return p1 + ' ' + p2.toUpperCase();
    });
    
    // Garantir ponto final se necess√°rio
    if (processed.length > 3 && !/[.!?]$/.test(processed)) {
      processed += '.';
    }

    console.log(`üîß Texto processado: "${text}" ‚Üí "${processed}"`);
    return processed;
  }

  // Calcular confian√ßa baseada na resposta do Whisper
  private calculateWhisperConfidence(response: any): number {
    // Se tem informa√ß√µes de segmentos, calcular m√©dia
    if (response.segments && response.segments.length > 0) {
      const avgConfidence = response.segments.reduce((sum: number, segment: any) => {
        return sum + (segment.avg_logprob || -0.5);
      }, 0) / response.segments.length;
      
      // Converter logprob para probabilidade (aproximada)
      const baseConfidence = Math.max(0.3, Math.min(0.95, Math.exp(avgConfidence)));
      
      // Ajustar confian√ßa baseada na qualidade do texto
      const textQuality = this.assessTextQuality(response.text);
      return Math.min(0.95, baseConfidence * textQuality);
    }
    
    // Baseado no tamanho e qualidade do texto
    const textLength = response.text.trim().length;
    const textQuality = this.assessTextQuality(response.text);
    
    let baseConfidence = 0.6;
    if (textLength > 50) baseConfidence = 0.9;
    else if (textLength > 20) baseConfidence = 0.8;
    else if (textLength > 10) baseConfidence = 0.7;
    
    return Math.min(0.95, baseConfidence * textQuality);
  }

  // Avaliar qualidade do texto transcrito
  private assessTextQuality(text: string): number {
    if (!text) return 0.1;
    
    let quality = 1.0;
    
    // Penalizar texto muito repetitivo
    const words = text.toLowerCase().split(/\s+/);
    const uniqueWords = new Set(words);
    const repetitionRatio = uniqueWords.size / words.length;
    if (repetitionRatio < 0.5) quality *= 0.7; // Muito repetitivo
    
    // Penalizar se s√≥ tem uma palavra repetida
    if (uniqueWords.size === 1 && words.length > 2) quality *= 0.3;
    
    // Bonificar se tem pontua√ß√£o apropriada
    if (/[.!?]/.test(text)) quality *= 1.1;
    
    // Penalizar ru√≠dos √≥bvios
    if (/\b(ah|eh|uh|hm|hmm)\b/gi.test(text)) quality *= 0.8;
    
    return Math.max(0.1, Math.min(1.0, quality));
  }

  // Integra√ß√£o com Google Speech-to-Text (futuro)
  private async transcribeWithGoogleSpeech(audioBuffer: Buffer): Promise<string> {
    // TODO: Implementar integra√ß√£o com Google Speech-to-Text API
    throw new Error('Google Speech integration not implemented yet');
  }

  // Configurar idioma
  public setLanguage(language: string): void {
    this.config.language = language;
    console.log(`üåç ASR language configurado: ${language}`);
  }

  // Configurar modelo
  public setModel(model: string): void {
    this.config.model = model;
    console.log(`ü§ñ ASR model configurado: ${model}`);
  }

  // Obter configura√ß√£o atual
  public getConfig(): ASRConfig {
    return { ...this.config };
  }

  // Obter status do servi√ßo
  public getStatus(): object {
    return {
      enabled: this.isEnabled,
      config: this.config,
      availableModels: ['whisper', 'google-speech'],
      supportedLanguages: ['pt-BR', 'en-US', 'es-ES']
    };
  }

  // Habilitar/desabilitar servi√ßo
  public setEnabled(enabled: boolean): void {
    this.isEnabled = enabled;
    console.log(`üé§ ASR Service ${enabled ? 'habilitado' : 'desabilitado'}`);
  }

  // Habilitar/desabilitar simula√ß√£o (para desenvolvimento)
  public setSimulationEnabled(enabled: boolean): void {
    this.enableSimulation = enabled;
    console.log(`üé≠ ASR Simula√ß√£o ${enabled ? 'habilitada' : 'desabilitada'}`);
  }

  // Verificar se simula√ß√£o est√° habilitada
  public isSimulationEnabled(): boolean {
    return this.enableSimulation;
  }

  /**
   * Trigger gera√ß√£o de sugest√µes ap√≥s nova transcri√ß√£o
   */
  private async triggerSuggestionGeneration(transcription: TranscriptionResult): Promise<void> {
    try {
      console.log(`ü§ñ Triggering suggestion generation for session ${transcription.sessionId}`);
      
      // Buscar informa√ß√µes da sess√£o
      const session = await db.getSession(transcription.sessionId);
      if (!session) {
        console.log('‚ö†Ô∏è Sess√£o n√£o encontrada para gera√ß√£o de sugest√µes');
        return;
      }

      // Buscar utterances recentes da sess√£o
      const utterances = await db.getSessionUtterances(transcription.sessionId);
      
      // Criar contexto para gera√ß√£o de sugest√µes
      const context = {
        sessionId: transcription.sessionId,
        patientName: 'Paciente', // TODO: Buscar nome real do paciente
        sessionDuration: this.calculateSessionDuration(session.created_at),
        consultationType: session.session_type || 'presencial',
        utterances: utterances.slice(-10), // √öltimas 10 utterances
        specialty: 'clinica_geral' // TODO: Determinar especialidade baseada no contexto
      };

      console.log(`üìä Context for suggestions: ${context.utterances.length} utterances, ${context.sessionDuration}min duration`);

      // Gerar sugest√µes de forma ass√≠ncrona (n√£o bloquear transcri√ß√£o)
      setImmediate(async () => {
        try {
          const suggestions = await suggestionService.generateSuggestions(context);
          if (suggestions && suggestions.suggestions.length > 0) {
            console.log(`ü§ñ ${suggestions.suggestions.length} sugest√µes geradas para sess√£o ${transcription.sessionId}`);
            
            // Notificar via WebSocket se dispon√≠vel
            await this.notifyWebSocketSuggestions(transcription.sessionId, suggestions.suggestions);
          } else {
            console.log(`ü§ñ Nenhuma sugest√£o gerada para sess√£o ${transcription.sessionId}`);
          }
        } catch (error) {
          console.error('‚ùå Erro ao gerar sugest√µes:', error);
        }
      });

    } catch (error) {
      console.error('‚ùå Erro no trigger de sugest√µes:', error);
    }
  }

  /**
   * Notifica sugest√µes via WebSocket
   */
  private async notifyWebSocketSuggestions(sessionId: string, suggestions: any[]): Promise<void> {
    try {
      // Tentar obter notifier do WebSocket
      const { SessionNotifier } = await import('../websocket/index');
      
      // Esta √© uma implementa√ß√£o simplificada - em produ√ß√£o, voc√™ teria uma refer√™ncia global ao notifier
      console.log(`üì° WebSocket notification preparada para sess√£o ${sessionId}: ${suggestions.length} sugest√µes`);
      
      // TODO: Implementar notifica√ß√£o real via WebSocket
      // Por enquanto, apenas log para debug
      
    } catch (error) {
      console.log('üì° WebSocket notifier n√£o dispon√≠vel - sugest√µes salvas no banco');
    }
  }

  /**
   * Calcula dura√ß√£o da sess√£o em minutos
   */
  private calculateSessionDuration(startTime: string): number {
    const start = new Date(startTime);
    const now = new Date();
    return Math.floor((now.getTime() - start.getTime()) / (1000 * 60));
  }
}

// Inst√¢ncia singleton do servi√ßo ASR
export const asrService = new ASRService();
