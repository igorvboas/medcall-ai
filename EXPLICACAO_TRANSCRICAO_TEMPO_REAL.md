# Explica√ß√£o: Sistema de Transcri√ß√£o em Tempo Real no Gateway

## üìã Vis√£o Geral

O sistema de transcri√ß√£o em tempo real funciona atrav√©s de um pipeline que captura √°udio do frontend, processa no gateway e envia para a API Whisper da OpenAI, retornando as transcri√ß√µes em tempo real via WebSocket.

---

## üîÑ Fluxo Completo da Transcri√ß√£o

### 1. **Captura de √Åudio no Frontend**

#### Para Consultas Presenciais (`PresentialCallRoom`)
- O frontend usa o hook `useAudioForker` para capturar √°udio de dois microfones (m√©dico e paciente)
- Cada chunk de √°udio √© enviado via WebSocket com o evento:
  - `presential:audio:doctor` - para √°udio do m√©dico
  - `presential:audio:patient` - para √°udio do paciente
- Os dados s√£o enviados como `Float32Array` convertido para array serializ√°vel

**Arquivo:** `apps/frontend/src/components/call/PresentialCallRoom.tsx`

```typescript
socket.emit(`presential:audio:${data.channel}`, {
  sessionId,
  audioData: Array.from(data.audioData), // Float32Array ‚Üí Array
  timestamp: data.timestamp,
  sampleRate: data.sampleRate
});
```

#### Para Consultas Online (`OnlineCallRoom`)
- Usa LiveKit para captura de √°udio
- Pode usar `useTranscriptionWebSocket` ou `RealtimeTranscription`
- Envia √°udio via eventos espec√≠ficos do LiveKit

---

### 2. **Recep√ß√£o no Gateway** (`audioHandler.ts`)

O gateway recebe os chunks de √°udio atrav√©s dos handlers:

```typescript
socket.on('presential:audio:doctor', (data: PresentialAudioData) => {
  // Converte array de volta para Float32Array
  const float32AudioData = new Float32Array(data.audioData);
  
  // Valida se h√° dados n√£o-zerados
  if (!hasNonZeroData) {
    console.warn('‚ö†Ô∏è DADOS ZERADOS RECEBIDOS');
    return;
  }
  
  // Cria chunk de √°udio e envia para processamento
  const audioChunk: AudioChunk = {
    sessionId,
    channel: 'doctor',
    audioData: float32AudioData,
    timestamp,
    sampleRate
  };
  
  audioProcessor.processAudioChunk(audioChunk);
});
```

**Arquivo:** `apps/gateway/src/websocket/audioHandler.ts` (linhas 24-88)

---

### 3. **Processamento de √Åudio** (`audioProcessor.ts`)

O `AudioProcessor` √© respons√°vel por:

#### a) **Voice Activity Detection (VAD)**
- Detecta se h√° voz no chunk usando RMS (Root Mean Square)
- Threshold configur√°vel: `vadThreshold = 0.05`
- S√≥ processa chunks com atividade de voz detectada

#### b) **Agrupamento em Frases Completas**
- **Modo atual:** Processa apenas frases completas (n√£o chunks parciais)
- Mant√©m buffers separados por canal (doctor/patient)
- Aguarda sil√™ncio de `1200ms` para finalizar uma frase
- M√°ximo de `15 segundos` por frase

#### c) **Normaliza√ß√£o de √Åudio**
- Normaliza o √°udio para 85% do m√°ximo para evitar clipping
- Melhora a qualidade da transcri√ß√£o

#### d) **Convers√£o para WAV**
- Converte `Float32Array` para buffer WAV completo (com header)
- Formato: PCM 16-bit, mono, 44.1kHz (ou sampleRate recebido)

**Arquivo:** `apps/gateway/src/services/audioProcessor.ts`

**Fluxo no AudioProcessor:**
```
processAudioChunk()
  ‚Üì
detectVoiceActivity() ‚Üí Se tem voz:
  ‚Üì
Adiciona ao phraseBuffer
  ‚Üì
Aguarda sil√™ncio de 1200ms
  ‚Üì
flushPhraseBuffer()
  ‚Üì
normalizeAudio()
  ‚Üì
float32ToWavBuffer()
  ‚Üì
emit('audio:processed', processedChunk)
```

---

### 4. **Envio para ASR Service** (`asrService.ts`)

Quando o `AudioProcessor` emite `audio:processed`, o handler em `audioHandler.ts` captura:

```typescript
const onAudioProcessed = (processedChunk: any) => {
  if (processedChunk.sessionId === sessionId) {
    // Enviar para ASR (Whisper)
    asrService.processAudio(processedChunk)
      .then((transcription) => {
        if (transcription) {
          // Prote√ß√£o contra duplica√ß√£o
          if (sentTranscriptionIds.has(transcription.id)) {
            return; // J√° foi enviado
          }
          
          sentTranscriptionIds.add(transcription.id);
          
          // Enviar para frontend via WebSocket
          notifier.emitTranscriptionUpdate(sessionId, utterance);
          
          // Trigger gera√ß√£o de sugest√µes
          triggerSuggestionGeneration(sessionId, utterance, notifier);
        }
      });
  }
};
```

**Arquivo:** `apps/gateway/src/websocket/audioHandler.ts` (linhas 293-350)

---

### 5. **Transcri√ß√£o com Whisper** (`asrService.ts`)

O `ASRService` processa o √°udio:

#### a) **Valida√ß√µes Pr√©-Envio**
- Verifica tamanho m√°ximo: 25MB
- Verifica dura√ß√£o m√°xima: 25 minutos
- Valida formato WAV (RIFF signature, headers, etc.)
- Verifica se h√° dados n√£o-zerados

#### b) **Chamada √† API Whisper**
```typescript
const formData = new FormData();
formData.append('model', 'whisper-1');
formData.append('file', audioBuffer, { filename: 'audio.wav' });
formData.append('language', 'pt');
formData.append('response_format', 'verbose_json');
formData.append('temperature', '0.0');
formData.append('prompt', 'Contexto m√©dico em portugu√™s brasileiro...');

const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
    ...formData.getHeaders()
  },
  body: formData
});
```

#### c) **P√≥s-Processamento**
- Remove ru√≠dos comuns (`[m√∫sica]`, `(tosse)`, etc.)
- Corrige espa√ßamento
- Capitaliza primeira letra
- Adiciona pontua√ß√£o se necess√°rio

#### d) **Fallback**
- Se Whisper n√£o estiver dispon√≠vel ou falhar, usa `generateRealBasedTranscription()`
- Gera transcri√ß√µes simuladas baseadas em caracter√≠sticas do √°udio

**Arquivo:** `apps/gateway/src/services/asrService.ts`

---

### 6. **Salvamento no Banco de Dados**

Ap√≥s receber a transcri√ß√£o do Whisper:

```typescript
await db.createUtterance({
  id: transcription.id,
  session_id: transcription.sessionId,
  speaker: transcription.speaker, // 'doctor' ou 'patient'
  text: transcription.text,
  confidence: transcription.confidence,
  start_ms: transcription.startTime,
  end_ms: transcription.endTime,
  is_final: transcription.is_final,
  created_at: transcription.timestamp
});
```

**Arquivo:** `apps/gateway/src/services/asrService.ts` (linhas 294-311)

---

### 7. **Envio para Frontend via WebSocket**

O `SessionNotifier` emite a transcri√ß√£o:

```typescript
notifier.emitTranscriptionUpdate(sessionId, utterance);
```

Que internamente faz:
```typescript
this.io.to(`session:${sessionId}`).emit('transcription:update', {
  sessionId,
  utterance: {
    id: transcription.id,
    speaker: transcription.speaker,
    text: transcription.text,
    timestamp: transcription.timestamp,
    confidence: transcription.confidence
  },
  timestamp: new Date().toISOString()
});
```

**Arquivo:** `apps/gateway/src/websocket/index.ts` (linhas 266-272)

---

### 8. **Recep√ß√£o no Frontend**

O frontend escuta o evento `transcription:update`:

```typescript
socket.on('transcription:update', (data: any) => {
  const newEntry: TranscriptionEntry = {
    id: data.id,
    speaker: data.speaker,
    text: data.text,
    timestamp: new Date(data.timestamp),
    confidence: data.confidence,
    isFinal: data.isFinal,
    language: data.language || 'pt-BR'
  };
  
  setTranscriptions(prev => {
    // Atualiza ou adiciona nova transcri√ß√£o
    const existingIndex = prev.findIndex(entry => 
      entry.id === newEntry.id || 
      (!newEntry.isFinal && entry.speaker === newEntry.speaker && !entry.isFinal)
    );
    
    if (existingIndex >= 0) {
      const updated = [...prev];
      updated[existingIndex] = newEntry;
      return updated;
    } else {
      return [...prev, newEntry];
    }
  });
});
```

**Arquivo:** `apps/frontend/src/components/livekit/RealtimeTranscription.tsx` (linhas 241-267)

---

## üõ°Ô∏è Prote√ß√µes e Controles

### 1. **Prote√ß√£o contra Duplica√ß√£o**
- `sentTranscriptionIds`: Set de IDs j√° enviados
- Verifica antes de enviar para evitar transcri√ß√µes duplicadas

### 2. **Prote√ß√£o contra Race Conditions**
- `globalProcessingLock`: Lock global por canal
- `processingInProgress`: Flag de processamento em andamento
- `lastProcessedTimestamp`: Prote√ß√£o temporal (m√≠nimo 8s entre processamentos)

### 3. **Valida√ß√£o de Dados**
- Verifica se dados n√£o est√£o zerados
- Valida formato WAV antes de enviar para Whisper
- Verifica tamanho e dura√ß√£o do √°udio

### 4. **Limpeza de Recursos**
- Remove listeners quando sess√£o termina
- Limpa buffers quando sess√£o √© finalizada
- Limpeza peri√≥dica do Set de IDs enviados

---

## ‚öôÔ∏è Configura√ß√µes Importantes

### AudioProcessor
- `vadThreshold`: 0.05 (sensibilidade de detec√ß√£o de voz)
- `minVoiceDurationMs`: 800ms (dura√ß√£o m√≠nima de voz)
- `phraseEndSilenceMs`: 1200ms (sil√™ncio que indica fim de frase)
- `maxPhraseLength`: 15000ms (m√°ximo 15s por frase)

### ASRService
- `model`: 'whisper-1'
- `language`: 'pt' (portugu√™s)
- `temperature`: 0.0 (m√°xima determina√ß√£o)
- `response_format`: 'verbose_json'

---

## üîç Pontos de Debug

O sistema tem logs detalhados em cada etapa:

1. **AUDIO_RECEPTION**: Quando √°udio √© recebido do frontend
2. **AUDIO_PROCESSING**: Quando come√ßa a processar √°udio
3. **TRANSCRIPTION_SEND**: Quando envia para Whisper
4. **TRANSCRIPTION_RECEIVED**: Quando recebe transcri√ß√£o do Whisper
5. **WEBSOCKET_SEND**: Quando envia para frontend via WebSocket

---

## üö® Problemas Comuns e Solu√ß√µes

### 1. **Dados Zerados Recebidos**
- **Causa:** Frontend n√£o est√° capturando √°udio corretamente
- **Solu√ß√£o:** Verificar permiss√µes de microfone, dispositivos selecionados

### 2. **Transcri√ß√µes Duplicadas**
- **Causa:** Race condition ou m√∫ltiplos listeners
- **Solu√ß√£o:** Sistema j√° tem prote√ß√µes, mas verificar se listeners est√£o sendo removidos corretamente

### 3. **Whisper Timeout**
- **Causa:** √Åudio muito grande ou API lenta
- **Solu√ß√£o:** Timeout de 30s configurado, verificar tamanho do √°udio

### 4. **Buffer WAV Inv√°lido**
- **Causa:** Convers√£o incorreta ou dados corrompidos
- **Solu√ß√£o:** Valida√ß√£o pr√©-envio j√° implementada

---

## üìä Fluxo Visual Simplificado

```
Frontend (Microfone)
    ‚Üì
WebSocket: presential:audio:doctor/patient
    ‚Üì
Gateway: audioHandler.ts
    ‚Üì
AudioProcessor: processAudioChunk()
    ‚Üì
VAD Detection ‚Üí Agrupa em frases
    ‚Üì
Normaliza ‚Üí Converte para WAV
    ‚Üì
Emit: audio:processed
    ‚Üì
ASRService: processAudio()
    ‚Üì
Whisper API (OpenAI)
    ‚Üì
P√≥s-processamento
    ‚Üì
Salva no Banco (utterances)
    ‚Üì
SessionNotifier: emitTranscriptionUpdate()
    ‚Üì
WebSocket: transcription:update
    ‚Üì
Frontend: Atualiza UI
```

---

## üîó Arquivos Principais

1. **Gateway:**
   - `apps/gateway/src/websocket/audioHandler.ts` - Handlers de √°udio
   - `apps/gateway/src/services/audioProcessor.ts` - Processamento de √°udio
   - `apps/gateway/src/services/asrService.ts` - Servi√ßo de transcri√ß√£o (Whisper)
   - `apps/gateway/src/websocket/index.ts` - SessionNotifier

2. **Frontend:**
   - `apps/frontend/src/components/call/PresentialCallRoom.tsx` - Captura de √°udio presencial
   - `apps/frontend/src/components/livekit/RealtimeTranscription.tsx` - Componente de transcri√ß√£o
   - `apps/frontend/src/hooks/useAudioForker.ts` - Hook para captura dual

---

## üìù Notas Importantes

1. **Modo de Processamento:** Atualmente processa apenas **frases completas**, n√£o chunks parciais
2. **Canais Separados:** Doctor e Patient s√£o processados separadamente
3. **Prote√ß√£o Temporal:** M√≠nimo de 8 segundos entre processamentos do mesmo canal
4. **Fallback:** Se Whisper falhar, usa transcri√ß√£o simulada baseada em caracter√≠sticas do √°udio
5. **Gera√ß√£o de Sugest√µes:** Ap√≥s cada transcri√ß√£o, triggera gera√ß√£o de sugest√µes de IA

---

Este documento explica o fluxo completo do sistema de transcri√ß√£o em tempo real. Se precisar de mais detalhes sobre alguma parte espec√≠fica, posso aprofundar!


